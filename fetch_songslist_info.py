#! /usr/bin/env python3
# -*- coding: utf8 -*-
# (c) Author: <kisfg@hotmail.com in 2024,2025>
# SPDX-LICENSE-IDENTIFIER: GPL2.0-ONLY
# Last modified at 2025/10/04 æ˜ŸæœŸå…­ 20:46:44
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU Library General Public
# License as published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# Library General Public License for more details.
#
# You should have received a copy of the GNU Library General Public
# License along with this library; if not, see <https://www.gnu.org/licenses/>.
"""
æœ¬ç¨‹åºåªç”¨äºä¸ªäººçˆ¬å–ç½‘æ˜“äº‘æ­Œå•çš„æ­Œæ›²åç­‰ä¿¡æ¯ä»¥ç”¨äºç•™å­˜å¤‡ä»½ã€‚
ä½¿ç”¨æ–¹å¼ï¼š
	é€šè¿‡æµè§ˆå™¨æä¾›è‡ªèº«ç™»å½•åå¾—åˆ°çš„ cookieã€æ­Œå• idï¼Œç„¶åè¿è¡Œè·å–ã€‚

ä¼šä¸´æ—¶ç”Ÿæˆå¯¹åº”èŒƒå›´çš„ä¸´æ—¶æ–‡ä»¶ä¾›å¤šçº¿ç¨‹ä¸´æ—¶å†™å…¥ã€‚

å‘½ä»¤è¡Œä¼ å‚å¿…è¦çš„å‚æ•°ï¼š
	- login_dummy: å‚€å„¡è´¦å·ã€‚å…¶ä¸­ token å’Œ cookie å¿…è¦ã€‚
	- songslist_author: ç›®æ ‡æ­Œå•ã€‚list-id å¿…è¦ï¼Œ
						ä¸”ç›®å‰ $(sys.platform)-backup-dir ä¹Ÿå¿…è¦ã€‚

æœ‰å…³è®¿é—®æ•°æ®åº“ã€ç»„ç»‡å»ºè¡¨çš„é€»è¾‘ä»åœ¨æ–½å·¥ä¸­ğŸš§ï¼Œä»ä¸èƒ½ä¿è¯å‰åå‘å…¼å®¹
é…ç½®å¥½ä»¥åè‡³å°‘ä¿è¯èƒ½è·‘ã€‚
"""

import sys
import random
import threading
from queue import Queue
from typing import Optional
from io import TextIOWrapper
from functools import partial
from multiprocessing import cpu_count
from concurrent.futures import ThreadPoolExecutor

from rich.progress import Progress, TaskID

from misc_utils.file_operator import (
	write_in_given_mode,
	append_from_ro_file,
	seek_to_remove_file,
	dir2file
)
from misc_utils.wrappers.err_wrap import seize_err_if_any
from misc_utils.http_meths.man import getter, poster
from misc_utils.opts.json.conf_reader import (
	PRIVATE_CONFIG,
	load_json_from_str,
	deserialize_json_or_die
)
from misc_utils.logger import DEBUG_LOGGER
from misc_utils.header import HEADER
from misc_utils.ref_types import (
	SongsResp,
	SongDetails,
	PlaylistDetail,
	PlaylistTrackInfo
)
from misc_utils.time_aux import (
	unix_time,
	unix_ts_to_time,
	US_TIME_FORMAT, 
	unix_ms
)
from misc_utils.args_loader import PARSER
from misc_utils.str_aux import dic2json_str
from misc_utils.opts.db.sqlite import DBfd
from misc_utils.diff_calc import myers_diff_comparer, DiffOp
from crypto_aux.manual_deobfus import netease_encryptor

# TODO: å¦‚æœçº¿ç¨‹æ‰§è¡Œè¿‡ç¨‹ä¸­é‡åˆ°ä»»ä½•å¼‚å¸¸ï¼Œç›´æ¥é€šçŸ¥å…¶å®ƒçº¿ç¨‹ç»“æŸï¼Œç®¡ç†çº¿ç¨‹çš„çˆ¶è¿›ç¨‹ç­‰å¾…å¹¶å¦¥å–„æ£€æŸ¥åå†é€€å‡º

_args = PARSER.parse_args()
del PARSER

_HTTPS: str = 'https://'
host: str = "music.163.com"
interface_prefix: str = f'{_HTTPS}interface.{host}'
# just GET
SONGSLIST_API: str = '/api/v6/playlist/detail?id='
SONG_SKIM_API: str = '/api/song/detail?ids='
# POST it
CONTENT_OF_SONG_PAPI: str = "/weapi/rep/ugc/song/get?csrf_token="
# useful urls
target_songslist_url: str = f'{interface_prefix}{SONGSLIST_API}'
target_songs_url: str = f"{interface_prefix}{SONG_SKIM_API}"
song_sniper_url: str = f'{_HTTPS}{host}{CONTENT_OF_SONG_PAPI}'
# concurrency control
_reveal_queue = Queue(maxsize=_args.threadpool_size)
_reveal_sema = threading.Semaphore(
	# å•æ ¸ CPU è¿˜æœ‰äººç”¨å—?
	min(cpu_count(), _args.threadpool_size//2)
)
# database handle
database_fd: Optional[DBfd] = DBfd(_args.database_path)


def song_sniper(sid: int, tok: str) -> any:
	"""
	è¿”å›ä¸€ä¸ªæ¯”è¾ƒåºå¤§çš„ç»“æ„
	"""
	global host, song_sniper_url
	origin_payload = dic2json_str({
		"songId": sid, "csrf_token": tok
	})
	assert 'Cookie' in HEADER and			\
			len(HEADER['Cookie']) != 0 and	\
			tok in HEADER['Cookie']
	changed_domains = {
		"Accept"			: "*/*",
		"Referer"			: f"{_HTTPS}{host}/reveal/song?songId={sid}&type=2",
		"Content-Type"		: "application/x-www-form-urlencoded",
		"Cookie"			: HEADER['Cookie'],
		"Host"				: host,
		"Origin"			: _HTTPS + host,
		"Priority"			: "u=1, i",
		"Sec-Fetch-Dest"	: "iframe",
		"TE"				: "trailers",
		"X-Requested-With"	: "XMLHttpRequest",
	}
	encrypted_payload, _ = netease_encryptor(origin_payload)
	ret = poster(
		url=song_sniper_url+tok,
		payload=encrypted_payload,
		alter_map=changed_domains
	)
	return ret


def _exam_hidden_sinfo(sid: int, tok: str) -> list:
	global _reveal_sema
	while True:
		_reveal_sema.acquire()
		threading.Event().wait(timeout=random.uniform(3, 6))
		# å› ä¸ºæ•°é‡é—®é¢˜è¿™é‡Œä¸å¤ªå¥½å®Œæ•´åœ°æ”¶é›†ä¸‹æ¥ï¼Œ
		# æ­¤å¤–éœ€è¦æ³¨æ„å¹¶å‘è¯·æ±‚çš„é¢‘ç‡å’Œè§„æ¨¡
		song_obj = song_sniper(sid, tok)
		if song_obj is None or len(song_obj.text) == 0:
			threading.Event().wait(timeout=random.uniform(3, 6))
			_reveal_sema.release()
			continue
		elif song_obj.status_code != 200:
			return [None, '', '', '', '', '']
		tmp = load_json_from_str(song_obj.text)
		if tmp is None or 'data' not in tmp:
			return [None, '', '', '', '', '']

		tdat = tmp['data']
		if 'playUrl' not in tdat:
			flag = None
		else:
			# TODO: è¿˜æœ‰ç§æƒ…å†µæ˜¯æœ‰ï¼Œä½† vip æ‰èƒ½å¬
			# ä½†æ˜¯è¿˜æ²¡ä»æ··æ·†jsçœ‹å‡ºä»€ä¹ˆä¸ºä»€ä¹ˆå®ƒæ‡‚å¾—æ— ç‰ˆæƒçš„æ‰€ä»¥ç„¶æ¥ï¼Œè¿™é‡Œå…ˆé¸½
			flag = tdat['playUrl'] is not None
		_jud = lambda x: x not in tdat or tdat[x] is None
		_muxer = lambda x: [] if _jud(x) else tdat[x]
		_texer = lambda x: x if isinstance(x, str) else ''
		lyric = _texer(_muxer('lyricContent'))
		trans = _texer(_muxer('transLyricContent'))
		composers = ', '.join([_['artistName'] for _ in _muxer('composeArtists')])
		arrangers = ', '.join([_['artistName'] for _ in _muxer('arrangeArtists')])
		lyricists = ', '.join([_['artistName'] for _ in _muxer('lyricArtists')])
		return [flag, lyric, trans, composers, arrangers, lyricists]


@seize_err_if_any()
def process_song_info_in_chunk(
	integrated_conf: dict,
	songs_detail: list[SongDetails],
	play_list: list[PlaylistTrackInfo]
) -> None:
	global _reveal_sema, _reveal_queue, database_fd

	st_pos = integrated_conf['st_pos']
	soid = integrated_conf['soid']
	bar_id = integrated_conf['taskID']
	for idx, a_song in enumerate(songs_detail):
		
		duration = a_song['duration']
		mins, ms = duration // 60_000, duration % 1_000
		sec = (duration - mins * 60_000 - ms) % 1_000
		final_dura = f'{mins}:{sec:02d}.{ms:03d}'
		del mins, sec, ms, duration

		song_id = a_song['id']
		# TODO: å¦‚æœæ•°æ®åº“é‡Œèƒ½æ‰¾è€Œä¸”ä¿¡æ¯éƒ½æ˜¯å…¨çš„ï¼Œå°±æ²¡å¿…è¦å»å†æ¬¡è¯·æ±‚ï¼Œæƒ³åŠæ³•ä¿è¯è¿™ä¸€ç‚¹å¹¶ç»™ä¸ªä¼˜åŒ–
		attacher = _exam_hidden_sinfo(song_id, integrated_conf['token'])
		copyleft = attacher[0]
		_lyc, _trl = attacher[1], attacher[2]
		arrangers, composers = attacher[3], attacher[4]
		lyricists = attacher[5]
		del attacher
		_reveal_sema.release()

		# ä¸€äº›å†…å®¹å¯ä»¥ç›´æ¥ä¼ ç»™æ•°æ®åº“æ¥åš
		album_publish_time: int = a_song['album']['publishTime']
		database_fd.insert_if_not_exist_else_renew(
			'albums', {'album_id': a_song['album']['id'] }, { 
				"company": a_song['album']['company'],
				'name': a_song['album']['name'],
				"publish_time": unix_ts_to_time(album_publish_time/1000)
			}
		)
		# æœ‰äº›æ­Œçš„æ­Œè¯å’Œç¿»è¯‘å¯èƒ½åœ¨åé¢ä¼šåŠ å…¥è¿›æ¥
		# æœ€å¥½è¿˜æ˜¯å…ˆæ’å…¥ç„¶åå†æ›´æ–°è¿™ä¸¤ä¸ªåŸŸ
		database_fd.update_item_in_tbl(
			"songs", { "song_id": song_id }, {
				'tr_pos': a_song['position'], 
				'album_id': a_song['album']['id'],
				"name": a_song['name'],
				'duration': final_dura,
				"lyrics": _lyc.replace('\n', '\\n'),
				"translatext": _trl.replace('\n', '\\n'),
				"vocals": ', '.join([v['name'] for v in a_song['artists']]), 
				"arrangers": arrangers, "lyricists": lyricists, 
				"composers": composers, "fetchable": copyleft, 
				"subtitle": ', '.join(a_song["alias"])
		})
		by_whom = play_list[idx]['uid']
		status, _ = database_fd.is_item_in_tbl(
			('user_id',), (by_whom,), 'users'
		)
		if not status:
			database_fd.insert_to_tbl(
				'users', { 'user_id': by_whom }
			)
		_tmp_ubid = database_fd.insert_if_not_exist_else_renew(
			'users_behaviors', { 'soid': soid, 'user_id': by_whom }
		)
		if _tmp_ubid is not None:
			ubid = _tmp_ubid
		else:
			status, row = database_fd.is_item_in_tbl(
				('soid', 'user_id'), (soid, by_whom),
				'users_behaviors'
			)
			# æ²¡æœ‰å‡ºç°è¿‡çš„é¡¹ç›®ï¼Œè¿™ä¸ªæ—¶å€™æ‰åº”è¯¥è¦æ’å…¥
			ubid = row["ubid"] \
				if status else database_fd.insert_if_not_exist_else_renew(
				'users_behaviors', { 'soid': soid, 'user_id': by_whom }
			)
			del status, row
		database_fd.update_item_in_tbl(
			'songs_status_in_songslists',
			{ 'song_id': song_id, 'ubid': ubid },
			{ "op_time": unix_ts_to_time(play_list[idx]['at']/1_000, US_TIME_FORMAT) }
		)
		_reveal_queue.put((bar_id, 0.6))


@seize_err_if_any()
def query_song_detail_in_range(
	bar_id: TaskID,
	integrated_conf: dict,
	l: int, r: int,
	play_list: list[PlaylistTrackInfo]
) -> None:
	"""
	{
		"res-song-get": {
			type: "GET",
			url: "/api/song/detail",
			format: function(m1x, e1x) {
				if (!!m1x.songs && m1x.songs.length > 0)
					m1x.song = m1x.songs[0];
				else
					m1x.song = bqK0x;
				delete m1x.songs;
				return xr5w(m1x, e1x)
			},
			filter: function(e1x) {
				e1x.data.ids = "[" + e1x.data.id + "]"
			}
		}
	}
	:return: None
	"""
	global target_songs_url, _reveal_sema, _reveal_queue
	# éšæœºç­‰ 1 ï½ 10 ç§’
	threading.Event().wait(timeout=random.randint(1, 10))
	res = getter(
		url=f'{target_songs_url}{[_s["id"] for _s in play_list]}',
		err_info=f'Catch an Exception in range [{l}, {r}): '
	)
	_check = lambda x: x is None or len(x.text) == 0 or x.status_code != 200
	songs_detail: Optional[SongsResp] = \
			None if _check(res) else	\
			load_json_from_str(res.text)
	del res

	nxt_merger = {
		'token': integrated_conf['token'],
		'taskID': bar_id,
		'st_pos': l,
		'soid': integrated_conf['soid'],
	}
	process_song_info_in_chunk(nxt_merger, songs_detail['songs'], play_list)
	_reveal_queue.put((bar_id, 0.4*(r-l)))
	return


def check_progress_state(prog_man: Progress) -> None:
	"""
	ç»ˆç«¯è¿›åº¦æ¡æ˜¾ç¤ºå‡½æ•°
	"""
	global _reveal_queue
	while not prog_man.finished:
		req: tuple[TaskID, int] = _reveal_queue.get()
		prog_man.update(req[0], advance=req[1])


@seize_err_if_any()
def songs_tasks_distributor(
	integrated_conf: dict,
	task_list: list[PlaylistTrackInfo],
	fn=query_song_detail_in_range
) -> None:
	"""
	è¿”å›å¹¶å‘æ‰§è¡Œå‰ï¼Œè®¡ç®—å¾—åˆ°çš„ä»»åŠ¡åˆ—è¡¨ã€‚
	"""
	global _reveal_queue
	workers = integrated_conf['worker_num']
	split_size = integrated_conf['split_size']

	lena = len(task_list)
	other, reminder = lena // split_size, lena % split_size
	tasks_queue = [(_*split_size, (_+1)*split_size) for _ in range(0, other)]
	tasks_queue.append((other*split_size, (other*split_size)+reminder))

	# ç¼©è¿›å¤šäº†å°±æˆäº†æ¨ªèººç€çš„shxt
	with Progress() as progress_man:
		_prog_rec = []
		for _ in range(len(tasks_queue)):
			tup = tasks_queue[_]
			_tmp = progress_man.add_task(
				# è¿™è¾ˆå­æŠŠåä¸‡é¦–æ­Œæ•´ç†åˆ°ä¸€ä¸ªå¸¸å¬çš„æ­Œå•é‡Œå¾ˆéš¾
				# ç‰¹åˆ«æ˜¯å®šæœŸçš„è§‚æµ‹ä¼šè®©è¿™ç§æ•°é‡çº§çš„æ•°æ®çˆ†ç‚¸å¼å¢é•¿
				f"Range:[{tup[0]:>4d},{tup[1]:>4d})",
				total=tup[1]-tup[0]
			)
			_prog_rec.append(_tmp)
		# ç”±äºåŠ äº†è¿›åº¦æ¡ï¼Œçº¿ç¨‹æ± éœ€è¦å¤šæ‰©ä¸€ä¸ªç»™è¿›åº¦æ¡çº¿ç¨‹
		nxt_merger = {
			'token': integrated_conf['token'],
			'soid': integrated_conf['soid'],
		}
		with ThreadPoolExecutor(max_workers=workers+1) as per_mission:
			per_mission.submit(check_progress_state, progress_man)
			for i, choice in enumerate(tasks_queue):
				per_mission.submit(
					fn, # è¦è°ƒç”¨çš„å‡½æ•°
					_prog_rec[i], nxt_merger,
					choice[0], choice[1],
					task_list[choice[0]:choice[1]]
				)
			del task_list


@seize_err_if_any()
def songslist_info_gen(
	tok: str,
	workersnum: int,
	crawl_conf: dict,
	split_size: int=100
) -> None:
	"""
	åˆ†æï¼šcore_52f85c5f5153a7880e60155739395661.js^[1]ä¸‹
	ç¬¬ 69 è¡ŒåŒ¿åå‡½æ•° (function()) é‡Œæœ‰

	{
		"res-playlist-get": {
			type: "GET",
			url: "/api/v6/playlist/detail",
			format: function(m1x, e1x) {
				var res = j1x.bsN0x(m1x);
				res.playlist = res.result;
				delete res.result;
				return xr5w(res, e1x)
			}
		}
	}

	åªéœ€ä¼  id åˆ—è¡¨å°±å¯ä»¥è·å–åˆ°åˆ—è¡¨å†…çš„æ‰€æœ‰æ­Œæ›²ã€‚
	åˆ—è¡¨é•¿åº¦ä¸èƒ½å®šå¾—å¤ªå¤§ï¼Œå¦åˆ™åæœè‡ªè´Ÿ

	- [1]: 2025/02/08: core_70d0eefb570184a2b62021346460be95.jsï¼Œ
	       åæ­£ç†è§£ä¸º core.jsã€‚
	"""
	global interface_prefix, target_songslist_url, database_fd

	response =getter(
		url=f"{target_songslist_url}{crawl_conf['list-id']}",
		err_info='fatal error while fetching songslist: '
	)

	if response is None:
		DEBUG_LOGGER.error('unable to fetch any songslist info')
		return
	elif response.status_code != 200:
		DEBUG_LOGGER.error(response.status_code, response.text)
		exit(1)

	serializer = response.text
	tmp: PlaylistDetail = deserialize_json_or_die(serializer, 'playlist')
	curr_time = unix_ms()
	del serializer, response

	songslist_updatetime = unix_ts_to_time(tmp["updateTime"]/1_000)
	songslist_birthday = unix_ts_to_time(tmp["createTime"]/1_000)
	slid = tmp["id"] # songslist_id
	track_cnt, share_cnt = tmp["trackCount"], tmp["shareCount"]
	likers_num, ref_hits = tmp["subscribedCount"], tmp["playCount"]
	description = tmp["description"]
	songslist_title = tmp["name"]

	creator_info = tmp["creator"]
	creator_id = creator_info["userId"]
	creator_name = creator_info["nickname"]
	creator_brief = creator_info['signature'].replace('\n', '\\n')
	del creator_info
	playlist = tmp['trackIds']
	del tmp
	print(f'For songslist-id {slid}')
	st_time = unix_time()
	# DB insert operations
	database_fd.insert_if_not_exist_else_renew(
		'users',
		{ 'user_id': creator_id },
		{ 'name': creator_name, 'brief': creator_brief }
	)
	database_fd.insert_if_not_exist_else_renew(
		'songslists',
		{ 'songslist_id': slid, 'user_id': creator_id },
		{ 'birthday': songslist_birthday }
	)
	soid = database_fd.insert_to_tbl(
		'songslists_observation', {
		'songslist_id': slid, 'user_id': creator_id, 
		'observation_time': unix_ts_to_time(curr_time/1000)
	})
	assert soid is not None
	def _songslist_attr_tbl_init(name: str, val: int | str) -> None:
		database_fd.insert_if_not_exist_else_renew(name+'_rec', { 'soid': soid }, { name: val })
	_songslist_attr_tbl_init('ref_hits', ref_hits)
	_songslist_attr_tbl_init('likers', likers_num)
	_songslist_attr_tbl_init('share_cnt', share_cnt)
	_songslist_attr_tbl_init('description', description)
	_songslist_attr_tbl_init('titles', songslist_title)
	ubid = database_fd.query_items_in_tbl(
		'users_behaviors',
		{ 'soid': soid, 'user_id': creator_id },
		'ubid', True
	)
	if ubid is None or len(ubid) == 0:
		ubid = database_fd.insert_to_tbl(
			'users_behaviors', 
			{ 'soid': soid, 'user_id': creator_id }
		)
	curr_idxs = [_["id"] for _ in playlist]
	sql_records = database_fd.query_items_in_tbl(
		"curr_songslists",
		{ "songslist_id": slid },
		'pos_val'
	)
	songslist_ids = [_["song_id"] for _ in sql_records] if sql_records is not None else []
	# å…ˆæŸ¥æ˜¯å¦å·²æœ‰æ­Œå•çš„ç»´æŠ¤è®°å½•
	# å¦‚æœæœ‰ç›´æ¥æ‹¿æ¥åƒgit diffä¸€æ ·æ¯”è¾ƒå·®å¼‚
	# éœ€è¦çœ‹çš„æ˜¯ä¸Šä¸€æ¬¡çš„å†…å®¹ï¼Ÿ
	# ä½†æ˜¯ä¸Šä¸€æ¬¡å†…å®¹æ˜¯å·®å¼‚ï¼Œå¦‚æœè¦çœ‹ï¼Œæ¯æ¬¡å°±éƒ½éœ€è¦åœ¨è¿™é‡Œç®—ç¬¬ä¸€æ¬¡çœ‹ç§¯ç´¯åˆ°ç°åœ¨çš„å·®å¼‚
	# ä¸å¦¨å°†è¿™ä¸ªå†…å®¹å•ç‹¬è®°å½•ä¸ºä½ç½®ä¿¡æ¯è¡¨
	# æ¯æ¬¡åªéœ€è¦è”åˆæŸ¥è¯¢ï¼Œè¿™æ ·å°±ä¸ç”¨æŠ˜è…¾æ¥æŠ˜è…¾å»äº†ï¼Œä¸å•ç‹¬è®°å½•
	len_ret = len(sql_records) if sql_records is not None else -1
	del sql_records, creator_id, creator_name, creator_brief
	del description, songslist_title, share_cnt, likers_num, ref_hits
	differs = myers_diff_comparer(songslist_ids, curr_idxs)
	songslist_ids = []
	for idx, edit_item in enumerate(differs):
		if edit_item[0] == DiffOp.keep:
			songslist_ids.append(edit_item[1][0])
			continue
		database_fd.insert_if_not_exist_else_renew('songs', { 'song_id': edit_item[1][0] })
		database_fd.insert_if_not_exist_else_renew(
			'songs_status_in_songslists', {
				'song_id': edit_item[1][0], 
				'ubid': ubid
			}, { f'{str(edit_item[0]).split(".")[-1]}_pos': edit_item[1][1]+1 }
		)
		if edit_item[0] == DiffOp.delete:
			continue
		songslist_ids.append(edit_item[1][0])
	del differs, curr_idxs
	for idx, item in enumerate(songslist_ids):
		if idx < len_ret:
			database_fd.update_item_in_tbl(
				"curr_songslists",
				{ "pos_val": idx+1, "songslist_id": slid }, 
				{ 'song_id': item }
			)
		else:
			database_fd.insert_to_tbl(
				'curr_songslists', 
				{ 'pos_val': idx+1, "songslist_id": slid, 'song_id': item }
			)
	for idx in range(len(songslist_ids), len_ret):
		database_fd.remove_item_in_tbl(
			'curr_songslists',
			{ 'pos_val': idx+1, 'songslist_id': slid }
		)
	ed_time = unix_time()
	# å·²ç»æœ‰çš„ä¸ç”¨å†å»è¯·ï¼Œåªè¯·æ±‚æ²¡æœ‰å¤‡ä»½è¿‡çš„
	rows = database_fd.query_null_items_in_tbl(
		'songs_status_in_songslists', 
		{ 'ubid': ubid },
		['delete_pos']
	)
	tmp = set([_['song_id'] for _ in rows ]) if rows is not None else set()
	adjust_playlist = []
	del rows, ubid, songslist_ids
	# é™¤éæ˜¯ä»€ä¹ˆä¹Ÿæ²¡æœ‰æ‰å®Œå…¨è·å–
	for p in playlist:
		if p['id'] not in tmp:
			continue
		adjust_playlist.append(p)
	del playlist

	integrated_conf = {
		'token': tok,
		'split_size': split_size,
		'soid': soid,
		'worker_num': workersnum
	}

	print(f'Time-consumption-on-setup-Tables: {ed_time - st_time}s. Begin Tasks...')
	songs_tasks_distributor(integrated_conf, adjust_playlist)


if __name__ == '__main__':
	spyon: str = _args.songslist_author
	dummy: str = _args.login_dummy
	workload = _args.threadpool_size
	assert 1 <= workload  # ä¸è®¾ä¸Šé™ï¼Œä½†è‡³å°‘è¦æœ‰ã€‚
	del _args

	dummy_conf: dict[str, str] = PRIVATE_CONFIG[dummy]
	victim_conf: dict[str, str] = PRIVATE_CONFIG[spyon]

	csrf_token, _cookie = dummy_conf["csrf_token"], dummy_conf["cookie"]
	del dummy_conf
	assert len(csrf_token) == 32 and f'__csrf={csrf_token}' in _cookie

	HEADER["Cookie"] = _cookie
	HEADER["Host"] = host
	HEADER["Referer"] = f"{_HTTPS}{host}/"
	HEADER["Connection"] = "keep-alive"
	songslist_info_gen(csrf_token, workload, victim_conf)
